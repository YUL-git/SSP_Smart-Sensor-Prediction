{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 율동공원팀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use python 3.11 in WSL  \n",
    "Nvidia RTX 1080 ti  \n",
    "- torch==2.0.1  \n",
    "- lightning==2.0.1  \n",
    "- wandb==0.17.0  \n",
    "- matplotlib==3.6.3  \n",
    "- numpy==11.24.0  \n",
    "- pandas==1.5.3  \n",
    "- scikit_learn==1.2.2  \n",
    "- seaborn==0.11.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy matplotlib seaborn scikit-learn torch pyarrow fastparquet py7zr lightning wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "plt.rc('font', family='Malgun Gothic')\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau, CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# import wandb\n",
    "# from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser(description=\"SSP_JY\")\n",
    "\n",
    "## DATA\n",
    "parser.add_argument(\"--train_path\", default=\"./data/valid_data\", type=str)\n",
    "parser.add_argument(\"--test_path\", default=\"./data/test_data\", type=str)\n",
    "parser.add_argument('--window_size', default=24, type=int)  # 수면 시간 고려하여 설정하였음\n",
    "parser.add_argument('--stride_size', default=1, type=int)  # 1시간 단위로 봄\n",
    "\n",
    "## MHA\n",
    "parser.add_argument('--num_head', default=8, type=int)\n",
    "parser.add_argument('--hid_dim', default=128, type=int)\n",
    "\n",
    "## TRAIN\n",
    "parser.add_argument('--optimizer', default=\"adamw\", type=str)\n",
    "parser.add_argument(\"--learning_rate\", default=1e-4, type=float)\n",
    "parser.add_argument(\"--weight_decay\", default=0, type=float)\n",
    "parser.add_argument('--scheduler', default=\"step\", type=str)\n",
    "parser.add_argument('--batch_size', default=16, type=int)\n",
    "parser.add_argument('--epochs', default=1000, type=int)\n",
    "parser.add_argument('--patience', default=100, type=int)\n",
    "\n",
    "parser.add_argument('--seed', default=42, type=int)\n",
    "parser.add_argument('--mixed_precision', default=32, type=int)\n",
    "parser.add_argument('--device', nargs='+', default=[0], type=int)\n",
    "parser.add_argument('--num_workers', default=0, type=int)\n",
    "\n",
    "args = parser.parse_args('')\n",
    "\n",
    "# wandb.init(config=args, name='SSP_JY(GAG)', project=\"ETRI_Baseline\")\n",
    "# wandb_logger = WandbLogger(name='SSP_JY(GAG)', project=\"ETRI_Baseline\")\n",
    "# wandb.config.update(args)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "CFG = {\n",
    "    \"WINDOW_SIZE\" : args.window_size,\n",
    "    \"STRIDE_SIZE\" : args.stride_size,\n",
    "    \"BATCH_SIZE\" : args.batch_size,\n",
    "    \"EPOCHS\"     : args.epochs,\n",
    "    \"PATIENCE\"   : args.patience,\n",
    "    \"SEED\"       : args.seed,\n",
    "    \"VALID_PATH\" : args.train_path,\n",
    "    \"TEST_PATH\"  : args.test_path,\n",
    "}\n",
    "\n",
    "def seed_everything(SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    L.seed_everything(SEED)\n",
    "\n",
    "torch.set_float32_matmul_precision('high') \n",
    "seed_everything(CFG['SEED'])\n",
    "\n",
    "idx = f\"{parser.description}_{device}\"\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 피실험자별 date 추출\n",
    "train_label = pd.read_csv('./data/valid_data/val_label.csv')\n",
    "train_label['date'] = pd.to_datetime(train_label['date'])\n",
    "\n",
    "test_label = pd.read_csv('./data/answer_sample.csv')\n",
    "test_label['date'] = pd.to_datetime(test_label['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_activity   = pd.read_parquet(os.path.join(CFG['VALID_PATH'], 'ch2024_val__m_activity.parquet.gzip'))\n",
    "df_gps        = pd.read_parquet(os.path.join(CFG['VALID_PATH'], 'ch2024_val__m_gps.parquet.gzip'))\n",
    "df_m_light    = pd.read_parquet(os.path.join(CFG['VALID_PATH'], 'ch2024_val__m_light.parquet.gzip'))\n",
    "df_pedo       = pd.read_parquet(os.path.join(CFG['VALID_PATH'], 'ch2024_val__w_pedo.parquet.gzip'))\n",
    "df_heart_rate = pd.read_parquet(os.path.join(CFG['VALID_PATH'], 'ch2024_val__w_heart_rate.parquet.gzip'))\n",
    "df_w_light    = pd.read_parquet(os.path.join(CFG['VALID_PATH'], 'ch2024_val__w_light.parquet.gzip'))\n",
    "df_usage      = pd.read_parquet(os.path.join(CFG['VALID_PATH'], 'ch2024_val__m_usage_stats.parquet.gzip'))\n",
    "df_ambience   = pd.read_parquet(os.path.join(CFG['VALID_PATH'], 'ch2024_val__m_ambience.parquet.gzip'))\n",
    "\n",
    "ts_activity   = pd.read_parquet(os.path.join(CFG['TEST_PATH'], 'ch2024_test_m_activity.parquet.gzip'))\n",
    "ts_gps        = pd.read_parquet(os.path.join(CFG['TEST_PATH'], 'ch2024_test_m_gps.parquet.gzip'))\n",
    "ts_m_light    = pd.read_parquet(os.path.join(CFG['TEST_PATH'], 'ch2024_test_m_light.parquet.gzip'))\n",
    "ts_pedo       = pd.read_parquet(os.path.join(CFG['TEST_PATH'], 'ch2024_test_w_pedo.parquet.gzip'))\n",
    "ts_heart_rate = pd.read_parquet(os.path.join(CFG['TEST_PATH'], 'ch2024_test_w_heart_rate.parquet.gzip'))\n",
    "ts_w_light    = pd.read_parquet(os.path.join(CFG['TEST_PATH'], 'ch2024_test_w_light.parquet.gzip'))\n",
    "ts_usage      = pd.read_parquet(os.path.join(CFG['TEST_PATH'], 'ch2024_test_m_usage_stats.parquet.gzip'))\n",
    "ts_ambience   = pd.read_parquet(os.path.join(CFG['TEST_PATH'], 'ch2024_test_m_ambience.parquet.gzip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 데이터 바꾸면서 불러오기\n",
    "df_m_acc = pd.read_parquet(os.path.join(CFG['VALID_PATH'],'ch2024_val__m_acc_part_4.parquet.gzip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가속도의 크기 계산\n",
    "df_m_acc['magnitude'] = np.sqrt(df_m_acc['x']**2 + df_m_acc['y']**2 + df_m_acc['z']**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macc = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'average_x': [],\n",
    "    'average_y': [],\n",
    "    'average_z': [],\n",
    "    'average_magnitude': []\n",
    "}\n",
    "\n",
    "# subject_id가 8인 경우만 처리\n",
    "id = 4 ################## 이거만 바꾸기\n",
    "user_timestamp = df_m_acc.loc[df_m_acc.subject_id == id, 'timestamp'].dt.date.unique()\n",
    "\n",
    "for day in user_timestamp:\n",
    "    day_start = pd.to_datetime(day)\n",
    "    day_end = day_start + pd.Timedelta(days=1)\n",
    "    hours = pd.date_range(start=day_start, end=day_end, freq='H', closed='left')\n",
    "    \n",
    "    for hour in hours:\n",
    "        user_data = df_m_acc.loc[(df_m_acc.subject_id == id) & (df_m_acc.timestamp.dt.floor('H') == hour), :]\n",
    "        \n",
    "        if not user_data.empty:\n",
    "            avg_x = user_data['x'].mean()\n",
    "            avg_y = user_data['y'].mean()\n",
    "            avg_z = user_data['z'].mean()\n",
    "            avg_magnitude = user_data['magnitude'].mean()\n",
    "        else:\n",
    "            avg_x = avg_y = avg_z = avg_magnitude = np.nan  # 데이터가 없는 경우 NaN으로 처리\n",
    "            \n",
    "        macc['subject_id'].append(id)\n",
    "        macc['hour'].append(hour)\n",
    "        macc['average_x'].append(avg_x)\n",
    "        macc['average_y'].append(avg_y)\n",
    "        macc['average_z'].append(avg_z)\n",
    "        macc['average_magnitude'].append(avg_magnitude)\n",
    "\n",
    "test_macc= pd.DataFrame(macc)\n",
    "# test_macc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_name = \"train_macc_4-2.csv\" ################### 파일명 바꾸기\n",
    "test_macc.to_csv(csv_file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## activity 활동성에 따른 재매핑\n",
    "\n",
    "activity_map = {\n",
    "    '8':5,\n",
    "    '1':4,\n",
    "    '7':3,\n",
    "    '3':2,\n",
    "    '0':1,\n",
    "    '4':0\n",
    "}\n",
    "\n",
    "df_activity.m_activity = df_activity.m_activity.map(activity_map)\n",
    "ts_activity.m_activity = ts_activity.m_activity.map(activity_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "\n",
    "ids  = [1,2,3,4]\n",
    "activity = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'activity': []\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = train_label.loc[train_label.subject_id == id, 'date'].dt.date\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        ## \n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_activity = df_activity.loc[(df_activity.subject_id == id) & (df_activity.timestamp.dt.floor('H') == hour), :]\n",
    "\n",
    "            if not user_activity.empty:\n",
    "                act = user_activity.m_activity.max()\n",
    "            else:\n",
    "                act = 0\n",
    "\n",
    "            activity['subject_id'].append(id)\n",
    "            activity['hour'].append(hour)\n",
    "            activity['activity'].append(act)\n",
    "\n",
    "train_activity = pd.DataFrame(activity)\n",
    "# train_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "ids = [5,6,7,8]\n",
    "activity = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'activity': []\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = test_label.loc[test_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_activity = ts_activity.loc[(ts_activity.subject_id == id) & (ts_activity.timestamp.dt.floor('H') == hour), :]\n",
    "            \n",
    "            if not user_activity.empty:\n",
    "                act = user_activity.m_activity.max()\n",
    "            else:\n",
    "                act = 0\n",
    "            \n",
    "            activity['subject_id'].append(id)\n",
    "            activity['hour'].append(hour)\n",
    "            activity['activity'].append(act)\n",
    "\n",
    "test_activity = pd.DataFrame(activity)\n",
    "# test_activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m_gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0  # 지구의 반지름 (km)\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def hourly_distance(df):\n",
    "    distances = haversine(df['latitude'].shift(), df['longitude'].shift(),\n",
    "                            df['latitude'], df['longitude'])\n",
    "    distances = distances.fillna(0)\n",
    "    return distances.sum()   # sum 을 통해 시간별 이동거리를 구하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "\n",
    "ids = [1,2,3,4]\n",
    "gps = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'distance': [],\n",
    "    'mean_altitude': [],\n",
    "    'mean_latitude': [],\n",
    "    'mean_longitude': [],\n",
    "    'mean_speed': []\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = train_label.loc[train_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_gps = df_gps.loc[(df_gps.subject_id == id) & (df_gps.timestamp.dt.floor('H') == hour), :]\n",
    "            \n",
    "            if not user_gps.empty:\n",
    "                distance = hourly_distance(user_gps)\n",
    "            else:\n",
    "                distance = 0.0\n",
    "            \n",
    "            mean_altitude = user_gps.altitude.mean()\n",
    "            mean_latitude = user_gps.latitude.mean()\n",
    "            mean_longitude = user_gps.longitude.mean()\n",
    "            mean_speed = user_gps.speed.mean()\n",
    "\n",
    "            gps['subject_id'].append(id)\n",
    "            gps['hour'].append(hour)\n",
    "            gps['distance'].append(distance)\n",
    "            gps['mean_altitude'].append(mean_altitude)\n",
    "            gps['mean_latitude'].append(mean_latitude)\n",
    "            gps['mean_longitude'].append(mean_longitude)\n",
    "            gps['mean_speed'].append(mean_speed)\n",
    "\n",
    "train_gps = pd.DataFrame(gps)\n",
    "# train_gps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "ids = [5,6,7,8]\n",
    "gps = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'distance': [],\n",
    "    'mean_altitude': [],\n",
    "    'mean_latitude': [],\n",
    "    'mean_longitude': [],\n",
    "    'mean_speed': []\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = test_label.loc[test_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_gps = ts_gps.loc[(ts_gps.subject_id == id) & (ts_gps.timestamp.dt.floor('H') == hour), :]\n",
    "            \n",
    "            if not user_gps.empty:\n",
    "                distance = hourly_distance(user_gps)\n",
    "            else:\n",
    "                distance = 0.0\n",
    "            \n",
    "            mean_altitude = user_gps.altitude.mean()\n",
    "            mean_latitude = user_gps.latitude.mean()\n",
    "            mean_longitude = user_gps.longitude.mean()\n",
    "            mean_speed = user_gps.speed.mean()\n",
    "            \n",
    "            gps['subject_id'].append(id)\n",
    "            gps['hour'].append(hour)\n",
    "            gps['distance'].append(distance)\n",
    "            gps['mean_altitude'].append(mean_altitude)\n",
    "            gps['mean_latitude'].append(mean_latitude)\n",
    "            gps['mean_longitude'].append(mean_longitude)\n",
    "            gps['mean_speed'].append(mean_speed)\n",
    "\n",
    "test_gps = pd.DataFrame(gps)\n",
    "# test_gps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "\n",
    "ids = [1,2,3,4]\n",
    "avg_m_light = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'max_light': [],\n",
    "    'mean_light': [],\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = train_label.loc[train_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_m_light = df_m_light.loc[(df_m_light.subject_id == id) & (df_m_light.timestamp.dt.floor('H') == hour), :]\n",
    "\n",
    "            if not user_m_light.empty:\n",
    "                max_light = user_m_light.m_light.max()\n",
    "                mean_light = user_m_light.m_light.mean()\n",
    "            else:\n",
    "                max_light = 0.0\n",
    "                mean_light = 0.0\n",
    "\n",
    "            avg_m_light['subject_id'].append(id)\n",
    "            avg_m_light['hour'].append(hour)\n",
    "            avg_m_light['max_light'].append(max_light)\n",
    "            avg_m_light['mean_light'].append(mean_light)\n",
    "\n",
    "train_m_light = pd.DataFrame(avg_m_light)\n",
    "# train_m_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "ids = [5,6,7,8]\n",
    "avg_m_light = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'max_light': [],\n",
    "    'mean_light': [],\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = test_label.loc[test_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_m_light = ts_m_light.loc[(ts_m_light.subject_id == id) & (ts_m_light.timestamp.dt.floor('H') == hour), :]\n",
    "\n",
    "            if not user_m_light.empty:\n",
    "                max_light = user_m_light.m_light.max()\n",
    "                mean_light = user_m_light.m_light.mean()\n",
    "            else:\n",
    "                max_light = 0.0\n",
    "                mean_light = 0.0\n",
    "\n",
    "            avg_m_light['subject_id'].append(id)\n",
    "            avg_m_light['hour'].append(hour)\n",
    "            avg_m_light['max_light'].append(max_light)\n",
    "            avg_m_light['mean_light'].append(mean_light)\n",
    "\n",
    "test_m_light = pd.DataFrame(avg_m_light)\n",
    "# test_m_light"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w_pedo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 건혁 인사이트 적용\n",
    "'''\n",
    ":Drop Columns: ['step_frequency', 'walking_steps','distance', 'speed']\n",
    "'''\n",
    "\n",
    "df_pedo = df_pedo[['subject_id', 'timestamp', 'burned_calories', 'running_steps', 'steps']]\n",
    "ts_pedo = ts_pedo[['subject_id', 'timestamp', 'burned_calories', 'running_steps', 'steps']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "\n",
    "ids = [1,2,3,4]\n",
    "avg_pedo = {\n",
    "    'subject_id': [],\n",
    "    'hour': []\n",
    "}\n",
    "mean_cols = ['mean_burned_calories', 'mean_running_steps', 'mean_steps']\n",
    "sum_cols = ['sum_burned_calories', 'sum_running_steps', 'sum_steps']\n",
    "user_avg_pedo = pd.DataFrame(columns=mean_cols)\n",
    "user_sum_pedo = pd.DataFrame(columns=sum_cols)\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = train_label.loc[train_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_pedo = df_pedo.loc[(df_pedo.subject_id == id) & (df_pedo.timestamp.dt.floor('H') == hour), :]\n",
    "            \n",
    "            if not user_pedo.empty:\n",
    "                user_pedo_mean = pd.DataFrame(user_pedo.iloc[:, 2:].mean()).T\n",
    "                user_pedo_mean.columns = mean_cols\n",
    "                user_pedo_sum  = pd.DataFrame(user_pedo.iloc[:, 2:].sum()).T\n",
    "                user_pedo_sum.columns = sum_cols\n",
    "            else:\n",
    "                user_pedo_mean = pd.DataFrame(np.zeros((1,3)), columns=mean_cols)\n",
    "                user_pedo_sum  = pd.DataFrame(np.zeros((1,3)), columns=sum_cols)\n",
    "            \n",
    "            avg_pedo['subject_id'].append(id)\n",
    "            avg_pedo['hour'].append(hour)\n",
    "            user_avg_pedo = pd.concat([user_avg_pedo, user_pedo_mean], axis=0)\n",
    "            user_sum_pedo = pd.concat([user_sum_pedo, user_pedo_sum], axis=0)\n",
    "\n",
    "user_avg_pedo.reset_index(drop=True, inplace=True)\n",
    "user_sum_pedo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "avg_pedo = pd.DataFrame(avg_pedo)\n",
    "train_pedo = pd.concat([avg_pedo, user_avg_pedo], axis=1)\n",
    "train_pedo = pd.concat([train_pedo, user_sum_pedo], axis=1)\n",
    "# train_pedo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "ids = [5,6,7,8]\n",
    "avg_pedo = {\n",
    "    'subject_id': [],\n",
    "    'hour': []\n",
    "}\n",
    "mean_cols = ['mean_burned_calories', 'mean_running_steps', 'mean_steps']\n",
    "sum_cols = ['sum_burned_calories', 'sum_running_steps', 'sum_steps']\n",
    "user_avg_pedo = pd.DataFrame(columns=mean_cols)\n",
    "user_sum_pedo = pd.DataFrame(columns=sum_cols)\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = test_label.loc[test_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end   = day_start + pd.Timedelta(days=1)\n",
    "        hours     = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_pedo = ts_pedo.loc[(ts_pedo.subject_id == id) & (ts_pedo.timestamp.dt.floor('H') == hour), :]\n",
    "            \n",
    "            if not user_pedo.empty:\n",
    "                user_pedo_mean = pd.DataFrame(user_pedo.iloc[:, 2:].mean()).T\n",
    "                user_pedo_mean.columns = mean_cols\n",
    "                user_pedo_sum  = pd.DataFrame(user_pedo.iloc[:, 2:].sum()).T\n",
    "                user_pedo_sum.columns = sum_cols\n",
    "            else:\n",
    "                user_pedo_mean = pd.DataFrame(np.zeros((1,3)), columns=mean_cols)\n",
    "                user_pedo_sum  = pd.DataFrame(np.zeros((1,3)), columns=sum_cols)\n",
    "            \n",
    "            avg_pedo['subject_id'].append(id)\n",
    "            avg_pedo['hour'].append(hour)\n",
    "            user_avg_pedo = pd.concat([user_avg_pedo, user_pedo_mean], axis=0)\n",
    "            user_sum_pedo = pd.concat([user_sum_pedo, user_pedo_sum], axis=0)\n",
    "\n",
    "user_avg_pedo.reset_index(drop=True, inplace=True)\n",
    "user_sum_pedo.reset_index(drop=True, inplace=True)\n",
    "\n",
    "avg_pedo = pd.DataFrame(avg_pedo)\n",
    "test_pedo = pd.concat([avg_pedo, user_avg_pedo], axis=1)\n",
    "test_pedo = pd.concat([test_pedo, user_sum_pedo], axis=1)\n",
    "# test_pedo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m_usage_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "\n",
    "ids = [1,2,3,4]\n",
    "avg_m_usage = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'app_total_use_time': [],\n",
    "    'app_mean_use_time': []\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = train_label.loc[train_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        total_times = 0\n",
    "\n",
    "        for hour in hours:\n",
    "            user_app = df_usage.loc[(df_usage.subject_id == id) & (df_usage.timestamp.dt.floor('H') == hour), :]\n",
    "            \n",
    "            for index, row in user_app.iterrows():\n",
    "                data = row['m_usage_stats']\n",
    "                if len(data):\n",
    "                    for item in data:\n",
    "                        if 'total_time' in item:\n",
    "                            total_times += item['total_time']\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            ## milisecond to minuts\n",
    "            total_times /= 60000\n",
    "            avg_m_usage['subject_id'].append(id)\n",
    "            avg_m_usage['hour'].append(hour)\n",
    "            avg_m_usage['app_total_use_time'].append(total_times)\n",
    "            \n",
    "            if len(user_app):\n",
    "                avg_m_usage['app_mean_use_time'].append(total_times / len(user_app))\n",
    "            else:\n",
    "                avg_m_usage['app_mean_use_time'].append(0.0)\n",
    "\n",
    "train_m_usage = pd.DataFrame(avg_m_usage)\n",
    "# train_m_usage.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "ids = [5,6,7,8]\n",
    "avg_m_usage = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'app_total_use_time': [],\n",
    "    'app_mean_use_time': []\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = test_label.loc[test_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        total_times = 0\n",
    "\n",
    "        for hour in hours:\n",
    "            user_app = ts_usage.loc[(ts_usage.subject_id == id) & (ts_usage.timestamp.dt.floor('H') == hour), :]\n",
    "            \n",
    "            for index, row in user_app.iterrows():\n",
    "                data = row['m_usage_stats']\n",
    "                if len(data):\n",
    "                    for item in data:\n",
    "                        if 'total_time' in item:\n",
    "                            total_times += item['total_time']\n",
    "                else:\n",
    "                    pass\n",
    "\n",
    "            ## milisecond to minuts\n",
    "            total_times /= 60000\n",
    "            avg_m_usage['subject_id'].append(id)\n",
    "            avg_m_usage['hour'].append(hour)\n",
    "            avg_m_usage['app_total_use_time'].append(total_times)\n",
    "            \n",
    "            if len(user_app):\n",
    "                avg_m_usage['app_mean_use_time'].append(total_times / len(user_app))\n",
    "            else:\n",
    "                avg_m_usage['app_mean_use_time'].append(0.0)\n",
    "\n",
    "test_m_usage = pd.DataFrame(avg_m_usage)\n",
    "# test_m_usage.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### m_ambience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "## top 1의 결과를 뽑음\n",
    "\n",
    "ids = [1,2,3,4]\n",
    "avg_m_ambience = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'max_ambience_cls': []\n",
    "}\n",
    "\n",
    "ambience_df = pd.DataFrame\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = train_label.loc[train_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            ambience_rate = 0\n",
    "            user_ambience = df_ambience.loc[(df_ambience.subject_id == id) & (df_ambience.timestamp.dt.floor('H') == hour), :]\n",
    "            \n",
    "            for index, row in user_ambience.iterrows():\n",
    "                data = row['ambience_labels']\n",
    "                if len(data):\n",
    "                    for item in data:\n",
    "                        if ambience_rate < float(item[-1]):\n",
    "                            ambience_rate = float(item[-1])\n",
    "                            ambience_name = item[0]\n",
    "\n",
    "                else:\n",
    "                    ambience_name = None\n",
    "                    pass\n",
    "\n",
    "            avg_m_ambience['subject_id'].append(id)\n",
    "            avg_m_ambience['hour'].append(hour)\n",
    "            avg_m_ambience['max_ambience_cls'].append(ambience_name)\n",
    "\n",
    "train_m_ambience = pd.DataFrame(avg_m_ambience)\n",
    "# train_m_ambience.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "ids = [5,6,7,8]\n",
    "avg_m_ambience = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'max_ambience_cls': [],\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = test_label.loc[test_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "\n",
    "        for hour in hours:\n",
    "            ambience_rate = 0\n",
    "            user_ambience = ts_ambience.loc[(ts_ambience.subject_id == id) & (ts_ambience.timestamp.dt.floor('H') == hour), :]\n",
    "            \n",
    "            for index, row in user_ambience.iterrows():\n",
    "                data = row['ambience_labels']\n",
    "                if len(data):\n",
    "                    for item in data:\n",
    "                        if ambience_rate < float(item[-1]):\n",
    "                            ambience_rate = float(item[-1])\n",
    "                            ambience_name = item[0]\n",
    "\n",
    "                else:\n",
    "                    ambience_name =None\n",
    "                    pass\n",
    "\n",
    "            avg_m_ambience['subject_id'].append(id)\n",
    "            avg_m_ambience['hour'].append(hour)\n",
    "            avg_m_ambience['max_ambience_cls'].append(ambience_name)\n",
    "\n",
    "test_m_ambience = pd.DataFrame(avg_m_ambience)\n",
    "# test_m_ambience.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w_heart_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "\n",
    "ids = [1,2,3,4]\n",
    "avg_hr = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'max_hr': [],\n",
    "    'mean_hr': []\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = train_label.loc[train_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_hr = df_heart_rate.loc[(df_heart_rate.subject_id == id) & (df_heart_rate.timestamp.dt.floor('H') == hour), :]\n",
    "\n",
    "            if not user_hr.empty:\n",
    "                hr_max = user_hr.heart_rate.max()\n",
    "                hr_mean = user_hr.heart_rate.mean()\n",
    "            else:\n",
    "                hr_max = 0.0\n",
    "                hr_mean = 0.0\n",
    "\n",
    "            avg_hr['subject_id'].append(id)\n",
    "            avg_hr['hour'].append(hour)\n",
    "            avg_hr['max_hr'].append(hr_max)\n",
    "            avg_hr['mean_hr'].append(hr_mean)\n",
    "\n",
    "train_hr = pd.DataFrame(avg_hr)\n",
    "# train_hr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "ids = [5,6,7,8]\n",
    "avg_hr = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'max_hr': [],\n",
    "    'mean_hr': []\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = test_label.loc[test_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_hr = ts_heart_rate.loc[(ts_heart_rate.subject_id == id) & (ts_heart_rate.timestamp.dt.floor('H') == hour), :]\n",
    "\n",
    "            if not user_hr.empty:\n",
    "                hr_max = user_hr.heart_rate.max()\n",
    "                hr_mean = user_hr.heart_rate.mean()\n",
    "            else:\n",
    "                hr_max = 0.0\n",
    "                hr_mean = 0.0\n",
    "\n",
    "            avg_hr['subject_id'].append(id)\n",
    "            avg_hr['hour'].append(hour)\n",
    "            avg_hr['max_hr'].append(hr_max)\n",
    "            avg_hr['mean_hr'].append(hr_mean)\n",
    "\n",
    "test_hr = pd.DataFrame(avg_hr)\n",
    "# test_hr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### w_light"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "\n",
    "ids = [1,2,3,4]\n",
    "avg_w_light = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'max_light': [],\n",
    "    'mean_light': []\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = train_label.loc[train_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_w_light = df_w_light.loc[(df_w_light.subject_id == id) & (df_w_light.timestamp.dt.floor('H') == hour), :]\n",
    "\n",
    "            if not user_w_light.empty:\n",
    "                max_light = user_w_light.w_light.max()\n",
    "                mean_light = user_w_light.w_light.mean()\n",
    "            else:\n",
    "                max_light = 0.0\n",
    "                mean_light = 0.0\n",
    "\n",
    "            avg_w_light['subject_id'].append(id)\n",
    "            avg_w_light['hour'].append(hour)\n",
    "            avg_w_light['max_light'].append(max_light)\n",
    "            avg_w_light['mean_light'].append(mean_light)\n",
    "\n",
    "train_w_light = pd.DataFrame(avg_w_light)\n",
    "# train_w_light.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test\n",
    "\n",
    "ids = [5,6,7,8]\n",
    "avg_w_light = {\n",
    "    'subject_id': [],\n",
    "    'hour': [],\n",
    "    'max_light': [],\n",
    "    'mean_light': []\n",
    "}\n",
    "\n",
    "for id in ids:\n",
    "    user_timestamp = test_label.loc[test_label.subject_id == id, 'date'].dt.floor('D')\n",
    "    \n",
    "    for day in user_timestamp:\n",
    "\n",
    "        day_start = pd.to_datetime(day)\n",
    "        day_end = day_start + pd.Timedelta(days=1)\n",
    "        hours = pd.date_range(start=day_start, end=day_end, freq='H', inclusive='left')\n",
    "        \n",
    "        for hour in hours:\n",
    "            user_w_light = ts_w_light.loc[(ts_w_light.subject_id == id) & (ts_w_light.timestamp.dt.floor('H') == hour), :]\n",
    "\n",
    "            if not user_w_light.empty:\n",
    "                max_light = user_w_light.w_light.max()\n",
    "                mean_light = user_w_light.w_light.mean()\n",
    "            else:\n",
    "                max_light = 0.0\n",
    "                mean_light = 0.0\n",
    "\n",
    "            avg_w_light['subject_id'].append(id)\n",
    "            avg_w_light['hour'].append(hour)\n",
    "            avg_w_light['max_light'].append(max_light)\n",
    "            avg_w_light['mean_light'].append(mean_light)\n",
    "\n",
    "test_w_light = pd.DataFrame(avg_w_light)\n",
    "# test_w_light.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.merge(train_activity, train_gps, on=['subject_id', 'hour'], how='left')\n",
    "train_data = pd.merge(train_data, train_m_light, on=['subject_id', 'hour'], how='left')\n",
    "train_data = pd.merge(train_data, train_pedo, on=['subject_id', 'hour'], how='left')\n",
    "train_data = pd.merge(train_data, train_hr, on=['subject_id', 'hour'], how='left')\n",
    "train_data = pd.merge(train_data, train_w_light, on=['subject_id', 'hour'], how='left')\n",
    "train_data = pd.merge(train_data, train_m_usage, on=['subject_id', 'hour'], how='left')\n",
    "train_data = pd.merge(train_data, train_m_ambience, on=['subject_id', 'hour'], how='left')\n",
    "\n",
    "train_data.shape\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.merge(test_activity, test_gps, on=['subject_id', 'hour'], how='left')\n",
    "test_data = pd.merge(test_data, test_m_light, on=['subject_id', 'hour'], how='left')\n",
    "test_data = pd.merge(test_data, test_pedo, on=['subject_id', 'hour'], how='left')\n",
    "test_data = pd.merge(test_data, test_hr, on=['subject_id', 'hour'], how='left')\n",
    "test_data = pd.merge(test_data, test_w_light, on=['subject_id', 'hour'], how='left')\n",
    "test_data = pd.merge(test_data, test_m_usage, on=['subject_id', 'hour'], how='left')\n",
    "test_data = pd.merge(test_data, test_m_ambience, on=['subject_id', 'hour'], how='left')\n",
    "\n",
    "test_data.shape\n",
    "# test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv(os.path.join(CFG['VALID_PATH'],'train_data2.csv'), index=False)\n",
    "test_data.to_csv(os.path.join(CFG['TEST_PATH'], 'test_data2.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(CFG['VALID_PATH'],'train_data2.csv'))\n",
    "test_data = pd.read_csv(os.path.join(CFG['TEST_PATH'], 'test_data2.csv'))\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = pd.read_csv(os.path.join(CFG['VALID_PATH'],'val_label.csv'))\n",
    "test_label  = pd.read_csv(os.path.join(CFG['TEST_PATH'],'answer_sample.csv'))\n",
    "\n",
    "train_label['date'] = pd.to_datetime(train_label['date'])\n",
    "test_label['date']  = pd.to_datetime(test_label['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(CFG['VALID_PATH'],'train_data2.csv'))\n",
    "test_data  = pd.read_csv(os.path.join(CFG['TEST_PATH'], 'test_data2.csv'))\n",
    "\n",
    "train_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {}\n",
    "\n",
    "for id in [1, 2, 3, 4]:\n",
    "    train_data_dict[f'train_macc_{id}'] = pd.read_csv(os.path.join(CFG['VALID_PATH'], f'train_macc_{id}-2.csv'))\n",
    "\n",
    "test_data_dict = {}\n",
    "\n",
    "for id in [5, 6, 7, 8]:\n",
    "    test_data_dict[f'test_macc_{id}'] = pd.read_csv(os.path.join(CFG['TEST_PATH'], f'test_macc_{id}-2.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = train_data_dict.keys()\n",
    "train_macc = pd.DataFrame()\n",
    "\n",
    "for key in keys:\n",
    "    train_macc = pd.concat([train_macc, train_data_dict[key]], axis=0)\n",
    "\n",
    "keys = test_data_dict.keys()\n",
    "test_macc = pd.DataFrame()\n",
    "\n",
    "for key in keys:\n",
    "    test_macc = pd.concat([test_macc, test_data_dict[key]], axis=0)\n",
    "\n",
    "train_macc.fillna(0, inplace=True)\n",
    "test_macc.fillna(0, inplace=True)\n",
    "\n",
    "train_macc.shape, test_macc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.merge(train_macc, on=['subject_id', 'hour'], how='left')\n",
    "test_data  = test_data.merge(test_macc, on=['subject_id', 'hour'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## categorical feature 처리\n",
    "\n",
    "def Info2Idx(df, cat_feat):\n",
    "    info2idx = {}\n",
    "    for f in cat_feat:\n",
    "        f_unique    = df[f].unique()\n",
    "        info2idx[f] = {k:v+1 for v, k in enumerate(f_unique)}\n",
    "    return info2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['hour'] = pd.to_datetime(train_data['hour'])\n",
    "test_data['hour']  = pd.to_datetime(test_data['hour'])\n",
    "\n",
    "train_data['time'] = train_data['hour'].dt.hour.astype(float)\n",
    "test_data['time']  = test_data['hour'].dt.hour.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['month'] = train_data['hour'].dt.month.astype(float)\n",
    "test_data['month']  = test_data['hour'].dt.month.astype(float)\n",
    "\n",
    "train_data['day'] = train_data['hour'].dt.dayofweek.astype(float)\n",
    "test_data['day']  = test_data['hour'].dt.dayofweek.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat = ['activity', 'month', 'max_ambience_cls']\n",
    "total_cat = pd.concat([train_data.loc[:, cat_feat], test_data.loc[:, cat_feat]], axis=0)\n",
    "\n",
    "info2idx = Info2Idx(total_cat, cat_feat)\n",
    "\n",
    "train_data[cat_feat] = train_data[cat_feat].apply(lambda x: x.map(info2idx[x.name]))\n",
    "test_data[cat_feat]  = test_data[cat_feat].apply(lambda x: x.map(info2idx[x.name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(X, Y, window_size=CFG['WINDOW_SIZE'], stride=CFG['STRIDE_SIZE'], for_train=True):\n",
    "    \n",
    "    if for_train:\n",
    "        ids = [1,2,3,4]\n",
    "        df_label = train_label.copy()\n",
    "    else:\n",
    "        ids = [5,6,7,8]\n",
    "        df_label = test_label.copy()\n",
    "    \n",
    "    train_sequences = []\n",
    "    valid_sequences = []\n",
    "\n",
    "    train_sequences_labels = []\n",
    "    valid_sequences_labels = []\n",
    "\n",
    "    train_cols = [\n",
    "        'activity', 'distance', 'mean_light_x', 'mean_burned_calories', 'mean_running_steps', 'mean_steps', \n",
    "        'mean_hr', 'mean_light_y', 'average_x', 'average_y', 'average_z', 'average_magnitude',\n",
    "        'app_total_use_time', 'max_ambience_cls',\n",
    "        ]\n",
    "    \n",
    "    label_cols = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3', 'S4']\n",
    "\n",
    "    for id in ids:\n",
    "\n",
    "        dates = df_label.loc[df_label.subject_id == id, 'date'].dt.date\n",
    "        \n",
    "        for idx, date in enumerate(dates):\n",
    "            \n",
    "            user_data  = X.loc[(X.subject_id == id) & (X.hour.dt.date == date), train_cols]\n",
    "            user_label = Y.loc[(Y.subject_id == id) & (Y.date.dt.date == date), label_cols]\n",
    "\n",
    "            user_data  = user_data.values\n",
    "            user_label = user_label.values\n",
    "            end = len(user_data) - window_size + 1\n",
    "            \n",
    "            for i in range(0, end, stride):\n",
    "                if (idx == 0) & for_train :\n",
    "                    valid_sequences.append(user_data[i : i + window_size, :])\n",
    "                    valid_sequences_labels.append(user_label)\n",
    "                elif for_train:\n",
    "                    train_sequences.append(user_data[i : i + window_size, :])\n",
    "                    train_sequences_labels.append(user_label)\n",
    "                else:\n",
    "                    train_sequences.append(user_data[i : i + window_size, :])\n",
    "                    train_sequences_labels.append(user_label)\n",
    "    \n",
    "    if for_train:\n",
    "        return np.array(train_sequences), np.array(train_sequences_labels), np.array(valid_sequences), np.array(valid_sequences_labels)\n",
    "    else:\n",
    "        return np.array(train_sequences), np.array(train_sequences_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_window_data, train_window_labels, valid_window_data, valid_window_labels = make_dataset(train_data, train_label, window_size=CFG['WINDOW_SIZE'], stride=CFG['STRIDE_SIZE'], for_train=True)\n",
    "test_window_data, test_window_labels  = make_dataset(test_data, test_label, window_size=CFG['WINDOW_SIZE'], stride=CFG['STRIDE_SIZE'], for_train=False)\n",
    "\n",
    "train_window_data.shape, train_window_labels.shape, valid_window_data.shape, valid_window_labels.shape, test_window_data.shape, test_window_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        X = torch.tensor(self.X[index], dtype=torch.float32)\n",
    "        \n",
    "        if self.Y is not None:\n",
    "            Y = torch.tensor(self.Y[index], dtype=torch.float32)\n",
    "            return X, Y\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUWithMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(GRUWithMultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.gru_in    = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.fc        = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads)\n",
    "        self.gru_out   = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        outputs, hidden = self.gru_in(x)                                # BxTx(Direction * H)\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim = 1))).unsqueeze(0)\n",
    "        outputs = self.fc(outputs)                                      # BxTxH\n",
    "\n",
    "        context_vec, _ = self.attention(outputs, outputs, outputs)      # BxTxH\n",
    "        \n",
    "        gru_input = context_vec[:, -1:, :]\n",
    "        gru_output, _ = self.gru_out(gru_input, hidden)                 # BxTxH\n",
    "        \n",
    "        return gru_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, args, input_size=8, hidden_size=args.hid_dim):\n",
    "        super(BaseModel, self).__init__()\n",
    "        \n",
    "        # 1D Convolution layers\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv1d(input_size, hidden_size//2, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm1d(hidden_size//2),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        \n",
    "        # Bidirectional GRU with Attention\n",
    "        self.gru_attention = GRUWithMultiHeadAttention(hidden_size//2, num_heads=args.num_head)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size//2, 7),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.permute(0, 2, 1)                  # BxTxF -> BxFxT\n",
    "        x = self.conv_block(x)                  # BxHxT'\n",
    "        x = x.permute(0, 2, 1)                  # BxT'xH\n",
    "        \n",
    "        gru_output = self.gru_attention(x)      # BxT'x(Direction * H)\n",
    "        output     = self.fc(gru_output[:, -1, :])  # Bx(Direction * H) -> Bx1\n",
    "\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseClassifier(L.LightningModule):\n",
    "    def __init__(self, backbone, args):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, x):\n",
    "        predictions = self.backbone(x)\n",
    "        return predictions\n",
    "\n",
    "    def step(self, batch):\n",
    "        x, y = batch\n",
    "        y_hat = self.backbone(x)\n",
    "        loss = nn.BCELoss()(y_hat, y.squeeze())\n",
    "        return loss, y, y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        self.log(\"test_mae\", loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        y_hat = self.forward(x)\n",
    "        return y_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if args.optimizer == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=args.learning_rate, momentum=0.9)\n",
    "        if args.optimizer == \"adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=args.learning_rate)\n",
    "        if args.optimizer == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "        \n",
    "        if args.scheduler == \"none\":\n",
    "            return optimizer\n",
    "        if args.scheduler == \"step\":\n",
    "            scheduler = StepLR(\n",
    "                optimizer=optimizer,\n",
    "                step_size=250,\n",
    "                gamma=0.05,\n",
    "            )\n",
    "            return [optimizer], [scheduler]\n",
    "        if args.scheduler == \"cosine\":\n",
    "            scheduler = CosineAnnealingLR(\n",
    "                optimizer=optimizer,\n",
    "                T_max=args.epochs,\n",
    "                eta_min=1e-6,\n",
    "            )\n",
    "            return [optimizer], [scheduler]\n",
    "        if args.scheduler == \"plateau\":\n",
    "            scheduler = ReduceLROnPlateau(\n",
    "                optimizer=optimizer,\n",
    "                mode=\"min\",\n",
    "                factor=0.1,\n",
    "                patience=2, # 2\n",
    "                verbose=False,\n",
    "            )\n",
    "            return {\"optimizer\":optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"val_loss\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_window_data, train_window_labels)\n",
    "valid_dataset = CustomDataset(valid_window_data, valid_window_labels)\n",
    "test_dataset  = CustomDataset(test_window_data, test_window_labels)\n",
    "\n",
    "train_loader  = DataLoader(train_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "valid_loader  = DataLoader(valid_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n",
    "test_loader   = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "##\n",
    "model   = BaseClassifier(BaseModel(args, input_size=train_window_data.shape[2]), args)\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"train_loss\", \n",
    "    patience=CFG['PATIENCE'], \n",
    "    mode=\"min\"\n",
    "    )\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(os.getcwd(),'saved'),\n",
    "    save_top_k=1,\n",
    "    verbose=False,\n",
    "    monitor='train_loss',\n",
    "    mode='min',\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=CFG[\"EPOCHS\"], accelerator=\"auto\", \n",
    "    enable_progress_bar=False,\n",
    "    enable_model_summary=False,\n",
    "    callbacks=[checkpoint_callback, early_stop_callback],\n",
    "    devices=args.device#, logger=wandb_logger,\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_loader, valid_loader)\n",
    "checkpoint_callback.best_model_path\n",
    "\n",
    "eval_dict = trainer.validate(model, dataloaders=valid_loader)[0]\n",
    "valid_loss = eval_dict[\"val_loss\"]\n",
    "\n",
    "y_valid_preds = trainer.predict(model, dataloaders=valid_loader)\n",
    "y_preds = trainer.predict(model, dataloaders=test_loader)\n",
    "\n",
    "print(f\"val_loss: {valid_loss}\")\n",
    "# wandb.log({'val_loss': valid_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_valid_pred = torch.vstack(y_valid_preds)\n",
    "final_valid_pred = final_valid_pred.cpu().numpy()\n",
    "final_valid_pred = np.where(final_valid_pred > 0.5, 1, 0)\n",
    "final_valid_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "total_f1 = 0\n",
    "fina_valid_real = valid_window_labels.squeeze(1)\n",
    "\n",
    "for i in range(7):\n",
    "\n",
    "    if i == 2:\n",
    "        weight = 1.0\n",
    "    else:\n",
    "        weight = 1.5\n",
    "    \n",
    "    f1 = f1_score(fina_valid_real[:,i], final_valid_pred[:,i])\n",
    "    f1 *= weight\n",
    "    total_f1 += f1\n",
    "\n",
    "    print(f\"f1_score_{i+1}: {f1}\")\n",
    "print(f\"total_f1: {total_f1}\")\n",
    "# wandb.log({'total_f1': total_f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = torch.vstack(y_preds)\n",
    "final_preds = final_preds.cpu().numpy()\n",
    "final_preds = np.where(final_preds > 0.5, 1, 0)\n",
    "final_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3', 'S4']\n",
    "for_barplot = pd.DataFrame(final_preds, columns=x)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(train_label.iloc[:, 2:].sum(0)/ 105, alpha=0.3, label='train', color='blue')\n",
    "sns.barplot(for_barplot.sum(0)/115, alpha=0.3, label='pred', color='red')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_barplot['subject_id'] = test_label.subject_id.values\n",
    "print(for_barplot.groupby('subject_id').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for_barplot.iloc[:, :].sum(0)/ 115, for_barplot.iloc[:, :].sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now()\n",
    "date = str(now.date()).replace('-', '_')\n",
    "time = str(now.time()).replace(':', '_')\n",
    "\n",
    "submmit = pd.read_csv(os.path.join(CFG['TEST_PATH'], 'answer_sample.csv'))\n",
    "submmit.iloc[:, 2:] = final_preds\n",
    "submmit.to_csv(f'./submission/submission_{idx}_{date}_{time}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submmit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "etri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
